# qwex.yaml - SSH + Slurm + Singularity + GDrive example
#
# Execution chain (outer → inner):
#   SSH → Slurm → Singularity → your command
#
# Storage:
#   - code: mounted directly or cloned from GitHub
#   - data: downloaded from GDrive at init
#   - runs: uploaded to GDrive on exit (logs, status, artifacts)

layers:
  ssh:
    type: ssh
    host: csc
    config: ~/.ssh/config

  # ─── Schedulers ───
  slurm:
    type: slurm
    partition: gpu
    time: "04:00:00"
    gpus: 1
    cpus: 4
    memory: 32G

  # ─── Containers ───
  singularity:
    type: singularity
    image: docker://pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

storage:
  # Direct mount - use existing local path
  code-local:
    type: mount
    source: .  # current directory
    path: /workspace/code

  # Git clone - for remote execution
  code-git:
    type: git
    repo: github.com/quatton/ssh-slurm-singularity-gdrive
    ref: main
    path: /workspace/code
    auth:
      type: gh

  data:
    type: gdrive
    folder_id: 1ABC123xyz
    path: /workspace/data
    readonly: true
    auth:
      type: service-account
      key: ~/.config/gdrive-sa.json

  runs:
    type: gdrive
    folder_id: 1XYZ789abc
    path: /workspace/runs
    sync: on-exit  # upload logs/artifacts when job finishes
    auth:
      type: service-account
      key: ~/.config/gdrive-sa.json
  
runners:
  # Full HPC: SSH → Slurm → Singularity
  hpc:
    layers:
      - ssh
      - slurm
      - singularity
    storage:
      source: code-git  # clone on remote
      data: data
      run: runs

  # Local Singularity (for testing)
  container:
    layers:
      - singularity
    storage:
      source: code-local  # direct mount

  # Default: no layers, just run locally
  # (implicit when no runner specified)
